@book{Ram-Hoo-Gra,
 author = {Ramsay, J. O. and Hooker, Giles and Graves, Spencer},
 title = {Functional Data Analysis with R and MATLAB},
 year = {2009},
 isbn = {0387981845, 9780387981840},
 edition = {1st},
 publisher = {Springer Publishing Company, Incorporated},
} 

@Manual{Rproject,
     title = {R: A Language and Environment for Statistical Computing},
     author = {{R Core Team}},
     organization = {R Foundation for Statistical Computing},
     address = {Vienna, Austria},
     year = {2017},
     url = {https://www.R-project.org/},
   }


@book{Ferraty2006,
 author = {Ferraty, Fr{\'e}d{\'e}ric and Vieu, Philippe},
 title = {Nonparametric Functional Data Analysis: Theory and Practice (Springer Series in Statistics)},
 year = {2006},
 isbn = {0387303693},
 publisher = {Springer-Verlag},
 address = {Berlin, Heidelberg},
} 

@article{Galeano2015,
author = {Pedro Galeano and Esdras Joseph and Rosa E. Lillo},
title = {The Mahalanobis Distance for Functional Data With Applications to Classification},
journal = {Technometrics},
volume = {57},
number = {2},
pages = {281-291},
year  = {2015},
publisher = {Taylor & Francis},
doi = {10.1080/00401706.2014.902774},

URL = { 
        https://doi.org/10.1080/00401706.2014.902774
    
},
eprint = { 
        https://doi.org/10.1080/00401706.2014.902774
    
}

}

@article {Tenenbaum2000,
	author = {Tenenbaum, Joshua B. and Silva, Vin de and Langford, John C.},
	title = {A Global Geometric Framework for Nonlinear Dimensionality Reduction},
	volume = {290},
	number = {5500},
	pages = {2319--2323},
	year = {2000},
	doi = {10.1126/science.290.5500.2319},
	publisher = {American Association for the Advancement of Science},
	abstract = {Scientists working with large volumes of high-dimensional data, such as global climate patterns, stellar spectra, or human gene distributions, regularly confront the problem of dimensionality reduction: finding meaningful low-dimensional structures hidden in their high-dimensional observations. The human brain confronts the same problem in everyday perception, extracting from its high-dimensional sensory inputs{\textemdash}30,000 auditory nerve fibers or 106 optic nerve fibers{\textemdash}a manageably small number of perceptually relevant features. Here we describe an approach to solving dimensionality reduction problems that uses easily measured local metric information to learn the underlying global geometry of a data set. Unlike classical techniques such as principal component analysis (PCA) and multidimensional scaling (MDS), our approach is capable of discovering the nonlinear degrees of freedom that underlie complex natural observations, such as human handwriting or images of a face under different viewing conditions. In contrast to previous algorithms for nonlinear dimensionality reduction, ours efficiently computes a globally optimal solution, and, for an important class of data manifolds, is guaranteed to converge asymptotically to the true structure.},
	issn = {0036-8075},
	URL = {https://science.sciencemag.org/content/290/5500/2319},
	eprint = {https://science.sciencemag.org/content/290/5500/2319.full.pdf},
	journal = {Science}
}



@ARTICLE{Dijkstra59anote,
    author = {E. W. Dijkstra},
    title = {A Note on Two Problems in Connexion with Graphs},
    journal = {NUMERISCHE MATHEMATIK},
    year = {1959},
    volume = {1},
    number = {1},
    pages = {269--271}
}


@article{Warshall1962,
 author = {Warshall, Stephen},
 title = {A Theorem on Boolean Matrices},
 journal = {J. ACM},
 issue_date = {Jan. 1962},
 volume = {9},
 number = {1},
 month = jan,
 year = {1962},
 issn = {0004-5411},
 pages = {11--12},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/321105.321107},
 doi = {10.1145/321105.321107},
 acmid = {321107},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@article{Floyd1962,
 author = {Floyd, Robert W.},
 title = {Algorithm 97: Shortest Path},
 journal = {Commun. ACM},
 issue_date = {June 1962},
 volume = {5},
 number = {6},
 month = jun,
 year = {1962},
 issn = {0001-0782},
 pages = {345--},
 url = {http://doi.acm.org/10.1145/367766.368168},
 doi = {10.1145/367766.368168},
 acmid = {368168},
 publisher = {ACM},
 address = {New York, NY, USA},
} 


@article{Genovese2014,
 ISSN = {00905364},
 URL = {http://www.jstor.org/stable/43556332},
 abstract = {We study the problem of estimating the ridges of a density function. Ridge estimation is an extension of mode finding and is useful for understanding the structure of a density. It can also be used to find hidden structure in point cloud data. We show that, under mild regularity conditions, the ridges of the kernel density estimator consistently estimate the ridges of the true density. When the data are noisy measurements of a manifold, we show that the ridges are close and topologically similar to the hidden manifold. To find the estimated ridges in practice, we adapt the modified mean-shift algorithm proposed by Ozertem and Erdogmus [J. Mach. Learn. Res. 12 (2011) 1249-1286]. Some numerical experiments verify that the algorithm is accurate.},
 author = {Christopher R. Genovese and Marco Perone-Pacifico and Isabella Verdinelli and Larry Wasserman},
 journal = {The Annals of Statistics},
 number = {4},
 pages = {1511--1545},
 publisher = {Institute of Mathematical Statistics},
 title = {NONPARAMETRIC RIDGE ESTIMATION},
 volume = {42},
 year = {2014}
}

@article{Kneip2008,
author = {Alois Kneip and James O Ramsay},
title = {Combining Registration and Fitting for Functional Models},
journal = {Journal of the American Statistical Association},
volume = {103},
number = {483},
pages = {1155-1165},
year  = {2008},
publisher = {Taylor & Francis},
doi = {10.1198/016214508000000517},

URL = { 
        https://doi.org/10.1198/016214508000000517
    
},
eprint = { 
        https://doi.org/10.1198/016214508000000517
    
}

}

@article{ChenMuller2012,
 ISSN = {00905364},
 URL = {http://www.jstor.org/stable/41713625},
 abstract = {For functional data lying on an unknown nonlinear low-dimensional space, we study manifold learning and introduce the notions of manifold mean, manifold modes of functional variation and of functional manifold components. These constitute nonlinear representations of functional data that complement classical linear representations such as eigenfunctions and functional principal components. Our manifold learning procedures borrow ideas from existing nonlinear dimension reduction methods, which we modify to address functional data settings. In simulations and applications, we study examples of functional data which lie on a manifold and validate the superior behavior of manifold mean and functional manifold components over traditional cross-sectional mean and functional principal components. We also include consistency proofs for our estimators under certain assumptions.},
 author = {Dong Chen and Hans-Georg Muller},
 journal = {The Annals of Statistics},
 number = {1},
 pages = {1--29},
 publisher = {Institute of Mathematical Statistics},
 title = {NONLINEAR MANIFOLD REPRESENTATIONS FOR FUNCTIONAL DATA},
 volume = {40},
 year = {2012}
}

@ARTICLE{LinYao2017,
       author = {{Lin}, Zhenhua and {Yao}, Fang},
        title = "{Functional Regression on Manifold with Contamination}",
      journal = {arXiv e-prints},
     keywords = {Statistics - Methodology},
         year = "2017",
        month = "Apr",
          eid = {arXiv:1704.03005},
        pages = {arXiv:1704.03005},
archivePrefix = {arXiv},
       eprint = {1704.03005},
 primaryClass = {stat.ME},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017arXiv170403005L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{Dimeglio2014,
 author = {Dimeglio, Chloe and Gallon, Santiago and Loubes, Jean-Michel and Maza, Elie},
 title = {A Robust Algorithm for Template Curve Estimation Based on Manifold Embedding},
 journal = {Comput. Stat. Data Anal.},
 issue_date = {February, 2014},
 volume = {70},
 month = feb,
 year = {2014},
 issn = {0167-9473},
 pages = {373--386},
 numpages = {14},
 url = {http://dx.doi.org/10.1016/j.csda.2013.09.030},
 doi = {10.1016/j.csda.2013.09.030},
 acmid = {2561681},
 publisher = {Elsevier Science Publishers B. V.},
 address = {Amsterdam, The Netherlands, The Netherlands},
 keywords = {Fr{\'e}chet median, Functional data analysis, Isomap},
} 

@article{ChenReiss2014,
author = {Chen, Huaihou and Reiss, Philip T. and Tarpey, Thaddeus},
title = {Optimally weighted L2 distance for functional data},
journal = {Biometrics},
volume = {70},
number = {3},
pages = {516-525},
keywords = {Coefficient of variation, Functional classification, Functional clustering, Penalized splines, Weighted  distance},
doi = {10.1111/biom.12161},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/biom.12161},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/biom.12161},
abstract = {Summary Many techniques of functional data analysis require choosing a measure of distance between functions, with the most common choice being distance. In this article we show that using a weighted distance, with a judiciously chosen weight function, can improve the performance of various statistical methods for functional data, including k-medoids clustering, nonparametric classification, and permutation testing. Assuming a quadratically penalized (e.g., spline) basis representation for the functional data, we consider three nontrivial weight functions: design density weights, inverse-variance weights, and a new weight function that minimizes the coefficient of variation of the resulting squared distance by means of an efficient iterative procedure. The benefits of weighting, in particular with the proposed weight function, are demonstrated both in simulation studies and in applications to the Berkeley growth data and a functional magnetic resonance imaging data set.},
year = {2014},
}

@article{LengMuller2005,
    author = {Leng, Xiaoyan and Muller, Hans-Georg},
    title = "{Classification using functional data analysis for temporal gene expression data}",
    journal = {Bioinformatics},
    volume = {22},
    number = {1},
    pages = {68-76},
    year = {2005},
    month = {10},
    abstract = "{Motivation: Temporal gene expression profiles provide an important characterization of gene function, as biological systems are predominantly developmental and dynamic. We propose a method of classifying collections of temporal gene expression curves in which individual expression profiles are modeled as independent realizations of a stochastic process. The method uses a recently developed functional logistic regression tool based on functional principal components, aimed at classifying gene expression curves into known gene groups. The number of eigenfunctions in the classifier can be chosen by leave-one-out cross-validation with the aim of minimizing the classification error.Results: We demonstrate that this methodology provides low-error-rate classification for both yeast cell-cycle gene expression profiles and Dictyostelium cell-type specific gene expression patterns. It also works well in simulations. We compare our functional principal components approach with a B-spline implementation of functional discriminant analysis for the yeast cell-cycle data and simulations. This indicates comparative advantages of our approach which uses fewer eigenfunctions/base functions. The proposed methodology is promising for the analysis of temporal gene expression data and beyond.Availability: MATLAB programs are available upon request.Contact:ileng@wfubmc.eduSupplementary information: Supplementary materials are available on the journal's website.}",
    issn = {1367-4803},
    doi = {10.1093/bioinformatics/bti742},
    url = {https://doi.org/10.1093/bioinformatics/bti742},
    eprint = {http://oup.prod.sis.lan/bioinformatics/article-pdf/22/1/68/16851583/bti742.pdf},
}



@article{Ozertem2011,
 author = {Ozertem, Umut and Erdogmus, Deniz},
 title = {Locally Defined Principal Curves and Surfaces},
 journal = {J. Mach. Learn. Res.},
 issue_date = {2/1/2011},
 volume = {12},
 month = jul,
 year = {2011},
 issn = {1532-4435},
 pages = {1249--1286},
 numpages = {38},
 url = {http://dl.acm.org/citation.cfm?id=1953048.2021041},
 acmid = {2021041},
 publisher = {JMLR.org},
} 

@inproceedings{Lin2014,
 author = {Lin, Binbin and Yang, Ji and He, Xiaofei and Ye, Jieping},
 title = {Geodesic Distance Function Learning via Heat Flow on Vector Fields},
 booktitle = {Proceedings of the 31st International Conference on International Conference on Machine Learning - Volume 32},
 series = {ICML'14},
 year = {2014},
 location = {Beijing, China},
 pages = {II-145--II-153},
 url = {http://dl.acm.org/citation.cfm?id=3044805.3044909},
 acmid = {3044909},
 publisher = {JMLR.org},
} 

@ARTICLE{ChenHo2015, 
author={Y. {Chen} and S. {Ho} and P. E. {Freeman} and C. R. {Genovese} and L. {Wasserman}}, 
journal={Monthly Notices of the Royal Astronomical Society}, 
title={Cosmic web reconstruction through density ridges: method and algorithm}, 
year={2015}, 
volume={454}, 
number={1}, 
pages={1140-1156}, 
keywords={methods: data analysis;methods: statistical;cosmology: observations;large-scale structure of Universe}, 
doi={10.1093/mnras/stv1996}, 
ISSN={0035-8711}, 
month={Sep.},}

@incollection{Joshi2007,
           month = {June},
          author = { S. Joshi and  A. Srivastava and  I.H. Jermyn},
       booktitle = {2007 IEEE Conference on Computer Vision and Pattern Recognition ; proceedings},
         address = {Piscataway, NJ},
           title = {Riemannian analysis of probability density functions with applications in vision.},
       publisher = {IEEE},
           pages = {1664--1671},
            year = {2007},
             url = {http://dro.dur.ac.uk/18442/},
        abstract = {Applications in computer vision involve statistically analyzing an important class of constrained, nonnegative functions, including probability density functions (in texture analysis), dynamic time-warping functions (in activity analysis), and re-parametrization or non-rigid registration functions (in shape analysis of curves). For this one needs to impose a Riemannian structure on the spaces formed by these functions. We propose a "spherical" version of the Fisher-Rao metric that provides closed-form expressions for geodesics and distances, and allows fast computation of sample statistics. To demonstrate this approach, we present an application in planar shape classification.}
}

@inbook{Diaconis2013,
address = "Beachwood, Ohio, USA",
author = "Diaconis, Persi and Holmes, Susan and Shahshahani, Mehrdad",
booktitle = "Advances in Modern Statistical Theory and Applications: A Festschrift in honor of Morris L. Eaton",
doi = "10.1214/12-IMSCOLL1006",
pages = "102--125",
publisher = "Institute of Mathematical Statistics",
series = "Collections",
title = "Sampling from a Manifold",
url = "https://doi.org/10.1214/12-IMSCOLL1006",
volume = "Volume 10",
year = "2013"
}

@book{Ramsay2005,
  title={Functional Data Analysis},
  author={Ramsay, J. and Ramsay, J. and Silverman, B.W. and Silverman, H.O.W.P.M.B.W. and Springer Science+Business Media},
  isbn={9780387400808},
  lccn={2005923773},
  series={Springer Series in Statistics},
  url={https://books.google.com.au/books?id=mU3dop5wY\_4C},
  year={2005},
  publisher={Springer}
}

@INPROCEEDINGS{Yeh2005, 
author={M. -. {Yeh} and I. -. {Lee} and G. {Wu} and Y. {Wu} and E. Y. {Chang}}, 
booktitle={2005 IEEE International Conference on Multimedia and Expo}, 
title={Manifold learning, a promised land or work in progress?}, 
year={2005}, 
volume={}, 
number={}, 
pages={4 pp.-}, 
keywords={image classification;pattern clustering;unsupervised learning;image retrieval;image clustering;image classification;manifold learning;realistic dataset;Kernel;Principal component analysis;Manifolds;Data engineering;Computer science;Image databases;Clustering algorithms;Image retrieval;Training data;Euclidean distance}, 
doi={10.1109/ICME.2005.1521631}, 
ISSN={1945-7871}, 
month={July},}
